{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beta Bank Customer Model\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Beta Bank customers are leaving little by little each month. The bank determined it's cheaper to save existing customers than to attract new ones. Knowing which customers will leave soon would be a valuable tool to help Beta Bank target the right customers before they decide to leave.\n",
    "\n",
    "### Prediction Model\n",
    "\n",
    "Beta Bank hopes a sufficent prediction model can be made. It's sufficiency depends if a an F1 score of at least 0.59 is attained. The AUR-ROC metric will also be compared. Client behaviour including whether they have terminated their contract or not has been provided. A description of data follows.\n",
    "\n",
    "### Data Description\n",
    "\n",
    "#### Features\n",
    "\n",
    "`RowNumber` — data string index\n",
    "\n",
    "`CustomerId` — unique customer identifier\n",
    "\n",
    "`Surname` — surname\n",
    "\n",
    "`CreditScore` — credit score\n",
    "\n",
    "`Geography` — country of residence\n",
    "\n",
    "`Gender` — gender\n",
    "\n",
    "`Age` — age\n",
    "\n",
    "`Tenure` — period of maturation for a customer’s fixed deposit (years)\n",
    "\n",
    "`Balance` — account balance\n",
    "\n",
    "`NumOfProducts` — number of banking products used by the customer\n",
    "\n",
    "`HasCrCard` — customer has a credit card\n",
    "\n",
    "`IsActiveMember` — customer’s activeness\n",
    "\n",
    "`EstimatedSalary` — estimated salary\n",
    "\n",
    "\n",
    "#### Target\n",
    "`Exited` — сustomer has left (if 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Process\n",
    "\n",
    "The process will follow these four steps:\n",
    "\n",
    "1. Prepare the data\n",
    "2. Examine the balance of classes\n",
    "3. Improve model quality\n",
    "4. Perform final testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read, Prepare and Pre-Process Data\n",
    "\n",
    "### To read:\n",
    "\n",
    "- Import packages \n",
    "- Save the dataframe\n",
    "\n",
    "### To prepare:\n",
    "- Check column names\n",
    "- Check data types \n",
    "- Identify missing values \n",
    "- Identify duplicates\n",
    "- Check binomial columns\n",
    "\n",
    "### To pre-process\n",
    "- Encode data based on learning algorithym types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import appropriate packages\n",
    "\n",
    "import pandas as pd # to save dataframe\n",
    "import numpy as np # to perform numerical operations\n",
    "import re # to use regular expressions\n",
    "from sklearn.preprocessing import LabelEncoder # for label encoding\n",
    "from sklearn.model_selection import train_test_split # to split data into training and testing sets\n",
    "from sklearn.ensemble import RandomForestClassifier # to train model with random forest\n",
    "from sklearn.linear_model import LogisticRegression # to train model with logistic regression\n",
    "from sklearn.metrics import accuracy_score # to calculate accuracy score\n",
    "import warnings # to ignore warnings related to logistical regression\n",
    "from sklearn.utils import shuffle # to shuffle data\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score # to calculate precision, recall and f1 scores\n",
    "from sklearn.metrics import roc_curve # to plot ROC curve\n",
    "import plotly.graph_objects as go # to plot ROC curve\n",
    "from sklearn.metrics import roc_auc_score # To calculate AUC score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to csv file\n",
    "df = pd.read_csv('./datasets/Churn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data\n",
    "\n",
    "#### Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0     2.0       0.00              1          1               1   \n",
       "1     1.0   83807.86              1          0               1   \n",
       "2     8.0  159660.80              3          1               0   \n",
       "3     1.0       0.00              2          0               0   \n",
       "4     2.0  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the first few rows of the dataframe and identify column names\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data column names are in CamelCase. Change this to snake_case as it is easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column names to snake case using regular expressions\n",
    "df.columns = [re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', col).lower() for col in df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   row_number        10000 non-null  int64  \n",
      " 1   customer_id       10000 non-null  int64  \n",
      " 2   surname           10000 non-null  object \n",
      " 3   credit_score      10000 non-null  int64  \n",
      " 4   geography         10000 non-null  object \n",
      " 5   gender            10000 non-null  object \n",
      " 6   age               10000 non-null  int64  \n",
      " 7   tenure            9091 non-null   float64\n",
      " 8   balance           10000 non-null  float64\n",
      " 9   num_of_products   10000 non-null  int64  \n",
      " 10  has_cr_card       10000 non-null  int64  \n",
      " 11  is_active_member  10000 non-null  int64  \n",
      " 12  estimated_salary  10000 non-null  float64\n",
      " 13  exited            10000 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Examine data types with .info()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that all data types are correct, except the 'tenure' column. Check the number of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tenure\n",
       "1.0     952\n",
       "2.0     950\n",
       "8.0     933\n",
       "3.0     928\n",
       "5.0     927\n",
       "7.0     925\n",
       "NaN     909\n",
       "4.0     885\n",
       "9.0     882\n",
       "6.0     881\n",
       "10.0    446\n",
       "0.0     382\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check unique values of 'tenure' column including NaN\n",
    "df['tenure'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values\n",
    "\n",
    "Notice that this column also contains missing values. It can safely be assumed that if no tenure exists, its value is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change missing values in 'tenure' column to 0\n",
    "df['tenure'] = df['tenure'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the column contains no missing values, we can convert it to an integer data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'tenure' column to integer type\n",
    "df['tenure'] = df['tenure'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicate Values\n",
    "\n",
    "Check duplicate values generally and across columns that should contain unique items. These include row_number and customer_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General duplicates: 0\n",
      "Row duplicates: 0\n",
      "Customer duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows generally\n",
    "print('General duplicates:',df.duplicated().sum())\n",
    "\n",
    "# Check for duplicates in row_number column\n",
    "print('Row duplicates:',df.duplicated(subset='row_number').sum())\n",
    "\n",
    "# Check for duplicate rows in customer_id column\n",
    "print('Customer duplicates:',df.duplicated(subset='customer_id').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As no duplicate rows are found, binomial columns can be checked.\n",
    "\n",
    "#### Binomial columns\n",
    "\n",
    "From observation, it seems that 'has_cr_card', 'is_active_member', and 'exited' columns are binomial categories. Ensure that this is the case by checking the number of unique values of these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_cr_card         2\n",
      "is_active_member    2\n",
      "exited              2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print number of unique values of df columns\n",
    "print(df[['has_cr_card','is_active_member', 'exited']].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The suspected binomial columns correctly contain two unique values. \n",
    "\n",
    "### Pre-Process Data\n",
    "\n",
    "The gender and geography columns contain categories as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique gender values: ['Female', 'Male']\n",
      "Unique gender values: ['France', 'Spain', 'Germany']\n"
     ]
    }
   ],
   "source": [
    "# Print unique values for gender and geography columns\n",
    "print('Unique gender values:', list(df['gender'].unique()))\n",
    "print('Unique gender values:', list(df['geography'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, sklearn learning algorithyms modules cannot comprehend string values. These values will need to be encoded so that the model may work with the data. The way they will be encoded will depend on the selected learning modules. These will be random forests and logistic regression.\n",
    "\n",
    "#### Gender\n",
    "\n",
    "As only two categories, 'Female' and 'Male', are given, either label encoding or one-hot encoding (OHE) may be used. This is because they will infer the same results which is ideal for both random trees and logistic regression algorithyms. \n",
    "\n",
    "For simplicity, label encoding will be used. Label encoding assigns values based on alphabetical order. Thus, the gender_label column will refer to:\n",
    "\n",
    "`female` as 0\n",
    "\n",
    "`male` as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LabelEncoder as encoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Encode 'gender' column\n",
    "df['gender_label'] = encoder.fit_transform(df['gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geography\n",
    "\n",
    "Unlike gender, the geography column has three values. This means encoding will differ for the two models. \n",
    "\n",
    "##### Random Forest: Label Encoder\n",
    "\n",
    "For models that use trees such as random forest, the label encoder will be used as it was above. The geography_label column will refer to:\n",
    "\n",
    "`France` as 0\n",
    "`Germany` as 1\n",
    "`Spain` as 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode geography for random forest models\n",
    "df['geography_label'] = encoder.fit_transform(df['geography'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression: One-Hot Encoding\n",
    "\n",
    "One-hot encoding (OHE) is a better use for the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make geography column lowercase to keep column names lowercase\n",
    "df['geography'] = df['geography'].str.lower()\n",
    "\n",
    "# One-hot encode geography for logistic regression models\n",
    "df = pd.get_dummies(df, columns=['geography'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance of Classes\n",
    "Check the balance of classes. Train the model without improvements to this balance. Note the findings.\n",
    "\n",
    "### Check Balance\n",
    "\n",
    "Check balance by examining the value counts of the target. Create a model that constantly predicts the more popular choice. Compare the two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exited\n",
      "0    79.63\n",
      "1    20.37\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Find value counts of the 'exited' column as a percentage\n",
    "print(df['exited'].value_counts(normalize=True)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.63\n"
     ]
    }
   ],
   "source": [
    "# Create model that predicts all customers will not churn\n",
    "target_pred_constant = pd.Series(0, index=df.index)\n",
    "\n",
    "# Print accuracy score for this model\n",
    "print(accuracy_score(df['exited'], target_pred_constant) *100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "A model that predicts that every customer won't churn will be accurate 79.63% of the time. A model must be created that does better than this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model without Improvements\n",
    "\n",
    "To train the model, the data must first be split. Three sets for the training, validation and test sets. Another split will seperate the features and target according to the requirements of the two learning algorithyms. The first will be a random forest, the second using logistic regression. \n",
    "\n",
    "\n",
    "### Split Data\n",
    "\n",
    "As we do not have a test set, the data will be split into 3 parts. These are for the training, validation and test sets. This split will occur in a 3:1:1 ratio as it keeps the validation and the test sets the same size.\n",
    "\n",
    "We can do this by first splitting the data in a 3:2 ratio to give two datasets. The first will serve as the test set. The second will be split again in 1:1 ratio to give the validation and test sets.\n",
    "\n",
    "Once this done, the datasets will be split into features and target.\n",
    "\n",
    "#### Training, Validation and Test Splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation with test \n",
    "df_train, df_valid_test = train_test_split(df, test_size=0.4, random_state=12345)\n",
    "\n",
    "# Split df_valid_test into validation and test\n",
    "df_valid, df_test = train_test_split(df_valid_test, test_size=0.5, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train size: (6000, 17)\n",
      "df_test size: (2000, 17)\n",
      "df_val size: (2000, 17)\n"
     ]
    }
   ],
   "source": [
    "# Confirm the size of splitted data sets\n",
    "print('df_train size:', df_train.shape)\n",
    "print('df_test size:', df_test.shape)\n",
    "print('df_val size:', df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "#### Description\n",
    "A random forest is a learning algorithym that trains groups of independent trees and makes decisions based on majority class prediction.\n",
    "\n",
    "#### Hyperparameters\n",
    "\n",
    "`max_depth`: Depth will be iterated for values 1 to 10.\n",
    "\n",
    "`n_estimators`: The number of trees that are built before voting takes place. This will be iterated from 10 to 100. \n",
    "\n",
    "`random_state`: '12345' will be used to produce consistent results.\n",
    "\n",
    "#### Process\n",
    "\n",
    "Features and the target will be determined using the appropriate preprocessing data. Accuracy scores with their associated parameters will be determined\n",
    "\n",
    "##### Features and Targets\n",
    "\n",
    "Select features by dropping those that arent relevant. These include 'row_number', 'customer_id', 'surname',  'geography_germany, 'geography_spain' and 'exited'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of columns to drop for feature selection for random forests\n",
    "rf_drop_columns = ['row_number', 'customer_id', 'surname','exited','gender','geography_germany','geography_spain']\n",
    "\n",
    "# Split the training data into features and target\n",
    "rf_features_train = df_train.drop(rf_drop_columns, axis=1)\n",
    "rf_target_train = df_train['exited']\n",
    "\n",
    "# Split the validation data into features and target\n",
    "rf_features_valid = df_valid.drop(rf_drop_columns, axis=1)\n",
    "rf_target_valid = df_valid['exited']\n",
    "\n",
    "# Split the test data into features and target\n",
    "rf_features_test = df_test.drop(rf_drop_columns, axis=1)\n",
    "rf_target_test = df_test['exited']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the best model: 86.45 %\n",
      "Found with 49 estimators and a depth of 8\n",
      "F1 score of the best model: 57.32 %\n"
     ]
    }
   ],
   "source": [
    "# Find the best accuracy score for the model with accomodating max_depth and n_estimators\n",
    "best_score = 0\n",
    "best_est = 0\n",
    "best_depth = 0\n",
    "f1 = 0\n",
    "\n",
    "# Create loop to find the best accuracy score with different max_depths and n_estimators\n",
    "for est in range(10, 51): # choose hyperparameter range\n",
    "    for depth in range(1, 11):\n",
    "        model = RandomForestClassifier(max_depth=depth, random_state=12345, n_estimators=est) # set number of trees\n",
    "        model.fit(rf_features_train, rf_target_train) # train model on training set\n",
    "        score = model.score(rf_features_valid, rf_target_valid) # calculate accuracy score on validation set\n",
    "        if score > best_score: # if accuracy score is better than previous best, save model and its accuracy score\n",
    "            best_score = score\n",
    "            best_est = est\n",
    "            best_depth = depth\n",
    "            f1 = f1_score(rf_target_valid, model.predict(rf_features_valid))\n",
    "\n",
    "print(\"Accuracy of the best model:\", round(best_score*100,2), \"%\")\n",
    "print(\"Found with\",best_est, \"estimators and a depth of\", best_depth)\n",
    "print(\"F1 score of the best model:\", round(f1*100,2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "#### Description\n",
    "\n",
    "Logistic regression may be used by assigning observations of features (calls, messages, usage, etc) as either positive or negative (Ultra or Smart plans) based on probability.\n",
    "\n",
    "##### Parameters\n",
    "\n",
    "`solver`: Sets the method for fitting the data. Each will be used and compared.\n",
    "\n",
    "`random_state`: '12345' will be used.\n",
    "\n",
    "#### Process\n",
    "\n",
    "Features and the target will be determined using the appropriate preprocessing data. Accuracy scores with their associated parameters will be determined\n",
    "\n",
    "##### Features and Targets\n",
    "\n",
    "Select features by dropping those that arent relevant. These include 'row_number', 'customer_id', 'surname',  'geography_label', and 'exited'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of columns to drop for feature selection for random forests\n",
    "lr_drop_columns = ['row_number', 'customer_id', 'gender', 'surname','exited','geography_germany','geography_spain']\n",
    "\n",
    "# Split the training data into features and target\n",
    "lr_features_train = df_train.drop(lr_drop_columns, axis=1)\n",
    "lr_target_train = df_train['exited']\n",
    "\n",
    "# Split the validation data into features and target\n",
    "lr_features_valid = df_valid.drop(lr_drop_columns, axis=1)\n",
    "lr_target_valid = df_valid['exited']\n",
    "\n",
    "# Split the test data into features and target\n",
    "lr_features_test = df_test.drop(lr_drop_columns, axis=1)\n",
    "lr_target_test = df_test['exited']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solver: newton-cg\n",
      "Accuracy on the validation set: 80.2 %\n",
      "F1 score on the validation set: 25.900000000000002 %\n",
      "\n",
      "Solver: lbfgs\n",
      "Accuracy on the validation set: 78.10000000000001 %\n",
      "F1 score on the validation set: 8.799999999999999 %\n",
      "\n",
      "Solver: liblinear\n",
      "Accuracy on the validation set: 78.2 %\n",
      "F1 score on the validation set: 8.4 %\n",
      "\n",
      "Solver: sag\n",
      "Accuracy on the validation set: 79.10000000000001 %\n",
      "F1 score on the validation set: 0.0 %\n",
      "\n",
      "Solver: saga\n",
      "Accuracy on the validation set: 79.10000000000001 %\n",
      "F1 score on the validation set: 0.0 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save solvers to a list\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "# Create a loop to find the best accuracy score with different solvers\n",
    "for solver in solvers:\n",
    "    # Ignore warnings\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        \n",
    "        # Train model\n",
    "        model = LogisticRegression(random_state=12345, solver=solver)\n",
    "        model.fit(lr_features_train, lr_target_train)\n",
    "        score_valid = round(model.score(lr_features_valid, lr_target_valid), 3) * 100\n",
    "        print('Solver:', solver)\n",
    "        print('Accuracy on the validation set:', score_valid, '%')\n",
    "        print('F1 score on the validation set:', round(f1_score(lr_target_valid, model.predict(lr_features_valid)), 3) * 100, '%')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "80% accuracy may look good, but is a bad metric to determine how good the model in comparison to the constant model. Only one Logistic regression model did better than the constant model on accuracy. Whilst the random forest did well with an accuracy of 86%, the constant model was only 6% behind. The high imbalance may have negatively affected the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve Model Quality\n",
    "\n",
    "To improve model quality, adjustments to class weights and thresholds will be made. Metrics aside from accuracy will be used improve model quality. These include precision, recall and F1 scores. \n",
    "\n",
    "Precision measures how many negative responses were found by the model when searching for positive responses. Recall shows how many positive answers were found by the model in relation to all positive responses. As these are both important, the F1 value combines the two and will be used as our main metric. The goal is to find one that exceeds 59%.\n",
    "\n",
    "Another good indicator of the effectiveness of our models is the area under curve and the receiver operating characteristic. These will be explained.\n",
    "\n",
    "### Class Weight Adjustment\n",
    "\n",
    "Machine learning algorithyms keep observations balanced by default. However, adjustments can be made through upscaling and downscaling.\n",
    "\n",
    "#### Upsampling\n",
    "\n",
    "Upsampling gives more weight to more important values by replicating observations. This requires 4 steps:\n",
    "\n",
    "1. Split data by target\n",
    "2. Duplicate appropriate observations\n",
    "3. Shuffle data\n",
    "4. Train models\n",
    "\n",
    "3.Improve the quality of the model. Make sure you use at least two approaches to fixing class imbalance. Use the training set to pick the best parameters. Train different models on training and validation sets. Find the best one. Briefly describe your findings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_zeros length: 4804\n",
      "target_ones length: 1196\n",
      "Ratio: 4.02\n",
      "Ratio Imbalanced\n"
     ]
    }
   ],
   "source": [
    "# 1. Split data by target\n",
    "\n",
    "# Split target_train data\n",
    "target_zeros = rf_target_train[rf_target_train == 0]\n",
    "target_ones = rf_target_train[rf_target_train == 1]\n",
    "\n",
    "# Split features data based on target data for both random forest and logistic regression\n",
    "rf_features_zeros = rf_features_train[rf_target_train == 0]\n",
    "rf_features_ones = rf_features_train[rf_target_train == 1]\n",
    "\n",
    "lr_features_zeros = lr_features_train[rf_target_train == 0]\n",
    "lr_features_ones = lr_features_train[rf_target_train == 1]\n",
    "\n",
    "# Print lengths for target zeros and target ones\n",
    "print('target_zeros length:', len(target_zeros))\n",
    "print('target_ones length:', len(target_ones))\n",
    "print('Ratio:', round(len(target_zeros)/len(target_ones), 2))\n",
    "print('Ratio Imbalanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations where the customer stayed: 4804\n",
      "Number of observations where the customer left: 4784\n",
      "Ratio: 1.0\n",
      "Ratio Balanced\n"
     ]
    }
   ],
   "source": [
    "# 2. Duplicate appropriate observations\n",
    "\n",
    "# Find ratio of target zeros to target ones\n",
    "ratio= int(len(target_zeros) / len(target_ones))\n",
    "\n",
    "# Duplicate target ones by ratio and concatenate for target for random forests\n",
    "rf_target_upsampled = pd.concat([target_zeros] + [target_ones] * ratio)\n",
    "\n",
    "# Duplicate target ones by ratio and concatenate for target for logistic regression\n",
    "lr_target_upsampled = pd.concat([target_zeros] + [target_ones] * ratio)\n",
    "\n",
    "# Repeat for features for random forests and logistic regression\n",
    "rf_features_upsampled = pd.concat([rf_features_zeros] + [rf_features_ones] * ratio)\n",
    "lr_features_upsampled = pd.concat([lr_features_zeros] + [lr_features_ones] * ratio)\n",
    "\n",
    "# Print lengths for target zeros and target ones\n",
    "print('Number of observations where the customer stayed:', len(target_zeros))\n",
    "print('Number of observations where the customer left:', len(target_ones) * ratio)\n",
    "print('Ratio:', round(len(target_zeros)/(len(target_ones) * ratio), 2))\n",
    "print('Ratio Balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Shuffle Data\n",
    "# Shuffle data for random forests\n",
    "rf_features_upsampled, rf_target_upsampled = shuffle(rf_features_upsampled, rf_target_upsampled, random_state=12345)\n",
    "\n",
    "# Repeat for logistic regression\n",
    "lr_features_upsampled, lr_target_upsampled = shuffle(lr_features_upsampled, lr_target_upsampled, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the best model: 61.7 %\n",
      "Found with 30 estimators and a depth of 9\n"
     ]
    }
   ],
   "source": [
    "# 4. Train Models\n",
    "\n",
    "# Train random forest model\n",
    "# Find the best F1 score for the model with accomodating max_depth and n_estimators\n",
    "\n",
    "best_score = 0\n",
    "best_est = 0\n",
    "best_depth = 0\n",
    "\n",
    "# Create loop to find the best accuracy score with different max_depths and n_estimators\n",
    "for est in range(10, 101, 10): # choose hyperparameter range\n",
    "    for depth in range(1, 11):\n",
    "        model = RandomForestClassifier(max_depth=depth, random_state=12345, n_estimators=est) # set number of trees\n",
    "        model.fit(rf_features_upsampled, rf_target_upsampled) # train model on training set\n",
    "        score = f1_score(rf_target_valid, model.predict(rf_features_valid)) # calculate f1 score on validation set\n",
    "        if score > best_score: # if f1 score is better than previous best, save model and its score\n",
    "            best_score = score\n",
    "            best_est = est\n",
    "            best_depth = depth\n",
    "\n",
    "print(\"F1 score of the best model:\", round(best_score*100,2), \"%\")\n",
    "print(\"Found with\", best_est, \"estimators and a depth of\", best_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solver: newton-cg\n",
      "Accuracy on the validation set: 69.0 %\n",
      "F1 score on the validation set: 27.900000000000002 %\n",
      "Solver: lbfgs\n",
      "Accuracy on the validation set: 65.4 %\n",
      "F1 score on the validation set: 28.299999999999997 %\n",
      "Solver: liblinear\n",
      "Accuracy on the validation set: 65.9 %\n",
      "F1 score on the validation set: 28.000000000000004 %\n",
      "Solver: sag\n",
      "Accuracy on the validation set: 49.6 %\n",
      "F1 score on the validation set: 29.7 %\n",
      "Solver: saga\n",
      "Accuracy on the validation set: 48.8 %\n",
      "F1 score on the validation set: 29.4 %\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression model\n",
    "# Save solvers to a list\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "# Create a loop to find the best accuracy score with different solvers\n",
    "for solver in solvers:\n",
    "    # Ignore warnings\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        \n",
    "        # Train model\n",
    "        model = LogisticRegression(random_state=12345, solver=solver)\n",
    "        model.fit(lr_features_upsampled, lr_target_upsampled)\n",
    "        score_valid = round(f1_score(lr_target_test, model.predict(lr_features_valid)), 3) * 100\n",
    "        print('Solver:', solver)\n",
    "        print('Accuracy on the validation set:', round(accuracy_score(lr_target_valid, model.predict(lr_features_valid)), 3) * 100, '%')\n",
    "        print('F1 score on the validation set:', score_valid, '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "Upsampling with the random trees model has produced a F1 of 62.18%. This is an improvement from 57.32% before balancing. Logistic regression has also seen increases in F1 scores to almost 30%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampling\n",
    "\n",
    "Instead of upsampling the number of observations where customers exited, downsampling the number of customers that stayed could also work. This works similarly to upsampling and follows these steps:\n",
    "\n",
    "1. Split data by target\n",
    "2. Randomly drop negative observations\n",
    "3. Shuffle data\n",
    "4. Train models\n",
    "\n",
    "As we already did the first step in upsampling, move onto the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations where the customer stayed: 1614.0\n",
      "Number of observations where the customer left: 1614\n",
      "Ratio: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 2. Randomly drop negative observations\n",
    "\n",
    "# Define fraction to drop\n",
    "fraction = len(target_ones)/len(target_zeros)\n",
    "\n",
    "\n",
    "# Drop negative observations for random forests\n",
    "rf_features_downsampled = pd.concat(\n",
    "    [rf_features_zeros.sample(frac=fraction, random_state=12345)]\n",
    "    + [rf_features_ones]) \n",
    "\n",
    "rf_target_downsampled = pd.concat(\n",
    "    [target_zeros.sample(frac=fraction, random_state=12345)]\n",
    "    + [target_ones])\n",
    "\n",
    "# Drop negative observations for logistic regression\n",
    "lr_features_downsampled = pd.concat(\n",
    "    [lr_features_zeros.sample(frac=fraction, random_state=12345)]\n",
    "    + [lr_features_ones])\n",
    "\n",
    "lr_target_downsampled = pd.concat(\n",
    "    [target_zeros.sample(frac=fraction, random_state=12345)]\n",
    "    + [target_ones])\n",
    "\n",
    "# Print lengths for target zeros and target ones\n",
    "print('Number of observations where the customer stayed:', len(target_zeros) * fraction)\n",
    "print('Number of observations where the customer left:', len(target_ones))\n",
    "print('Ratio:', round(len(target_zeros) * fraction/len(target_ones), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2410, 3228]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/lizzd/TripleTen/Sprint 8 - Supervised Learning/project/betabank-ml/betabank-ml.ipynb Cell 56\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lizzd/TripleTen/Sprint%208%20-%20Supervised%20Learning/project/betabank-ml/betabank-ml.ipynb#Y324sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 3. Shuffle data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lizzd/TripleTen/Sprint%208%20-%20Supervised%20Learning/project/betabank-ml/betabank-ml.ipynb#Y324sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Shuffle data for random forests\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lizzd/TripleTen/Sprint%208%20-%20Supervised%20Learning/project/betabank-ml/betabank-ml.ipynb#Y324sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m rf_features_downsampled, rf_target_downsampled \u001b[39m=\u001b[39m shuffle(rf_features_downsampled, rf_target_downsampled, random_state\u001b[39m=\u001b[39m\u001b[39m12345\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lizzd/TripleTen/Sprint%208%20-%20Supervised%20Learning/project/betabank-ml/betabank-ml.ipynb#Y324sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Repeat for logistic regression\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lizzd/TripleTen/Sprint%208%20-%20Supervised%20Learning/project/betabank-ml/betabank-ml.ipynb#Y324sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m lr_features_downsampled, lr_target_downsampled \u001b[39m=\u001b[39m shuffle(lr_features_downsampled, lr_target_downsampled, random_state\u001b[39m=\u001b[39m\u001b[39m12345\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/qtconsole/lib/python3.11/site-packages/sklearn/utils/__init__.py:691\u001b[0m, in \u001b[0;36mshuffle\u001b[0;34m(random_state, n_samples, *arrays)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshuffle\u001b[39m(\u001b[39m*\u001b[39marrays, random_state\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, n_samples\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    626\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Shuffle arrays or sparse matrices in a consistent way.\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \n\u001b[1;32m    628\u001b[0m \u001b[39m    This is a convenience alias to ``resample(*arrays, replace=False)`` to do\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[39m      array([0, 1])\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 691\u001b[0m     \u001b[39mreturn\u001b[39;00m resample(\n\u001b[1;32m    692\u001b[0m         \u001b[39m*\u001b[39marrays, replace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, n_samples\u001b[39m=\u001b[39mn_samples, random_state\u001b[39m=\u001b[39mrandom_state\n\u001b[1;32m    693\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/qtconsole/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    215\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/qtconsole/lib/python3.11/site-packages/sklearn/utils/__init__.py:577\u001b[0m, in \u001b[0;36mresample\u001b[0;34m(replace, n_samples, random_state, stratify, *arrays)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39melif\u001b[39;00m (max_n_samples \u001b[39m>\u001b[39m n_samples) \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m replace):\n\u001b[1;32m    572\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    573\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot sample \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m out of arrays with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m when replace is False\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    574\u001b[0m         \u001b[39m%\u001b[39m (max_n_samples, n_samples)\n\u001b[1;32m    575\u001b[0m     )\n\u001b[0;32m--> 577\u001b[0m check_consistent_length(\u001b[39m*\u001b[39marrays)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     \u001b[39mif\u001b[39;00m replace:\n",
      "File \u001b[0;32m~/anaconda3/envs/qtconsole/lib/python3.11/site-packages/sklearn/utils/validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    405\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    408\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    410\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2410, 3228]"
     ]
    }
   ],
   "source": [
    "# 3. Shuffle data\n",
    "# Shuffle data for random forests\n",
    "rf_features_downsampled, rf_target_downsampled = shuffle(rf_features_downsampled, rf_target_downsampled, random_state=12345)\n",
    "\n",
    "# Repeat for logistic regression\n",
    "lr_features_downsampled, lr_target_downsampled = shuffle(lr_features_downsampled, lr_target_downsampled, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the best model: 60.38 %\n",
      "Found with 90 estimators and a depth of 6\n"
     ]
    }
   ],
   "source": [
    "# 4. Train Models\n",
    "\n",
    "# Train random forest model\n",
    "# Find the best F1 score for the model with accomodating max_depth and n_estimators\n",
    "\n",
    "best_score = 0\n",
    "best_est = 0\n",
    "best_depth = 0\n",
    "\n",
    "# Create loop to find the best accuracy score with different max_depths and n_estimators\n",
    "for est in range(10, 101, 10): # choose hyperparameter range\n",
    "    for depth in range(1, 11):\n",
    "        model = RandomForestClassifier(max_depth=depth, random_state=12345, n_estimators=est) # set number of trees\n",
    "        model.fit(rf_features_downsampled, rf_target_downsampled) # train model on training set\n",
    "        score = f1_score(rf_target_valid, model.predict(rf_features_valid)) # calculate f1 score on validation set\n",
    "        if score > best_score: # if f1 score is better than previous best, save model and its score\n",
    "            best_score = score\n",
    "            best_est = est\n",
    "            best_depth = depth\n",
    "\n",
    "print(\"F1 score of the best model:\", round(best_score*100,2), \"%\")\n",
    "print(\"Found with\", best_est, \"estimators and a depth of\", best_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solver: newton-cg\n",
      "Accuracy on the validation set: 69.0 %\n",
      "F1 score on the validation set: 27.800000000000004 %\n",
      "Solver: lbfgs\n",
      "Accuracy on the validation set: 65.0 %\n",
      "F1 score on the validation set: 28.1 %\n",
      "Solver: liblinear\n",
      "Accuracy on the validation set: 65.10000000000001 %\n",
      "F1 score on the validation set: 28.499999999999996 %\n",
      "Solver: sag\n",
      "Accuracy on the validation set: 47.699999999999996 %\n",
      "F1 score on the validation set: 29.799999999999997 %\n",
      "Solver: saga\n",
      "Accuracy on the validation set: 47.699999999999996 %\n",
      "F1 score on the validation set: 29.599999999999998 %\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression model\n",
    "\n",
    "# Create a loop to find the best F1 score with different solvers\n",
    "for solver in solvers:\n",
    "    # Ignore warnings\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        \n",
    "        # Train model\n",
    "        model = LogisticRegression(random_state=12345, solver=solver)\n",
    "        model.fit(lr_features_downsampled, lr_target_downsampled)\n",
    "        score_valid = round(f1_score(lr_target_test, model.predict(lr_features_valid)), 3) * 100\n",
    "        print('Solver:', solver)\n",
    "        print('Accuracy on the validation set:', round(accuracy_score(lr_target_valid, model.predict(lr_features_valid)), 3) * 100, '%')\n",
    "        print('F1 score on the validation set:', score_valid, '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "Downsampling produced similar, but slightly less ideal results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Adjustments\n",
    "\n",
    "Threshold adjustments may help improve the F1 score of the logistic regression models. By default this level is set to 0.5 as it asigns a probability of 0.5 to each target observation outcome. However, this can be changed iterating through threshold values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best solver: saga\n",
      "Threshold = 0.22 | Precision = 0.322, Recall = 0.577, F1 = 41.30 %\n"
     ]
    }
   ],
   "source": [
    "# Create a loop to find the best f1 score with different solvers\n",
    "for solver in solvers:\n",
    "    # Ignore warnings\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        model = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "        model.fit(lr_features_train, lr_target_train)\n",
    "        probabilities_valid = model.predict_proba(lr_features_valid)\n",
    "        probabilities_one_valid = probabilities_valid[:, 1]\n",
    "\n",
    "        best_f1 = 0\n",
    "        best_threshold = 0\n",
    "        best_precision = 0\n",
    "        best_recall = 0\n",
    "        best_solver = ''\n",
    "\n",
    "        for threshold in np.arange(0, 0.5, 0.02):\n",
    "            predicted_valid = probabilities_one_valid > threshold\n",
    "            precision = precision_score(lr_target_valid, predicted_valid)\n",
    "            recall = recall_score(lr_target_valid, predicted_valid)\n",
    "            f1 = precision * recall * 2 / (precision + recall)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "                best_precision = precision\n",
    "                best_recall = recall\n",
    "                best_solver = solver\n",
    "                \n",
    "print('Best solver:', best_solver)\n",
    "\n",
    "print(\n",
    "    'Threshold = {:.2f} | Precision = {:.3f}, Recall = {:.3f}, F1 = {:.2f} %'.format(\n",
    "        best_threshold, best_precision, best_recall, best_f1 * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "By changing threshold levels, the model was optimized for precision and recall. The best solver, 'saga', produced an F1 score of 41.3%. However, this is still lower than the random forests model.\n",
    "\n",
    "### AUC-ROC\n",
    "\n",
    "By iterating through the threshold, a curve that shows the relationship between the true positive rate (TPR) and false positive rate (FPR) can be made. A random model should show a consistent rise as each increases. However, a model that is better will show a curve that rises above. Once this is calculated, the area under the curve can be used to show how effective it may be. An area equal to one shows that the model is perfect, whereas a model that shows lower than 0.5 is worse than a random model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "deeppink"
         },
         "mode": "lines",
         "name": "High quality model",
         "type": "scatter",
         "x": [
          0,
          0.0006321112515802782,
          0.0037926675094816687,
          0.0037926675094816687,
          0.006321112515802781,
          0.006321112515802781,
          0.00695322376738306,
          0.00695322376738306,
          0.008217446270543615,
          0.008217446270543615,
          0.008849557522123894,
          0.008849557522123894,
          0.012642225031605562,
          0.012642225031605562,
          0.01580278128950695,
          0.01580278128950695,
          0.01643489254108723,
          0.01643489254108723,
          0.01706700379266751,
          0.01706700379266751,
          0.020227560050568902,
          0.020227560050568902,
          0.021491782553729456,
          0.021491782553729456,
          0.022756005056890013,
          0.022756005056890013,
          0.025916561314791402,
          0.025916561314791402,
          0.02654867256637168,
          0.02654867256637168,
          0.02718078381795196,
          0.02718078381795196,
          0.0316055625790139,
          0.0316055625790139,
          0.03350189633375474,
          0.03350189633375474,
          0.03413400758533502,
          0.03413400758533502,
          0.035398230088495575,
          0.035398230088495575,
          0.03666245259165613,
          0.03666245259165613,
          0.03729456384323641,
          0.03729456384323641,
          0.03792667509481669,
          0.03792667509481669,
          0.04108723135271808,
          0.04108723135271808,
          0.042351453855878636,
          0.042351453855878636,
          0.04361567635903919,
          0.04361567635903919,
          0.04424778761061947,
          0.04424778761061947,
          0.04487989886219975,
          0.04487989886219975,
          0.0461441213653603,
          0.0461441213653603,
          0.04804045512010114,
          0.04804045512010114,
          0.05309734513274336,
          0.05309734513274336,
          0.05625790139064475,
          0.05625790139064475,
          0.05815423514538559,
          0.05815423514538559,
          0.05941845764854614,
          0.05941845764854614,
          0.06131479140328698,
          0.06131479140328698,
          0.061946902654867256,
          0.061946902654867256,
          0.06257901390644753,
          0.06257901390644753,
          0.0632111251580278,
          0.0632111251580278,
          0.06763590391908976,
          0.06763590391908976,
          0.06826801517067003,
          0.06826801517067003,
          0.06890012642225031,
          0.06890012642225031,
          0.07079646017699115,
          0.07079646017699115,
          0.07142857142857142,
          0.07142857142857142,
          0.07332490518331226,
          0.07332490518331226,
          0.07901390644753477,
          0.07901390644753477,
          0.08154235145385588,
          0.08154235145385588,
          0.08343868520859671,
          0.08343868520859671,
          0.084070796460177,
          0.084070796460177,
          0.08470290771175727,
          0.08470290771175727,
          0.08596713021491782,
          0.08596713021491782,
          0.0865992414664981,
          0.0865992414664981,
          0.08786346396965866,
          0.08786346396965866,
          0.09039190897597978,
          0.09039190897597978,
          0.09102402022756005,
          0.09102402022756005,
          0.09481668773704172,
          0.09481668773704172,
          0.09924146649810367,
          0.09924146649810367,
          0.10050568900126422,
          0.10050568900126422,
          0.10366624525916561,
          0.10366624525916561,
          0.10493046776232617,
          0.10493046776232617,
          0.10556257901390645,
          0.10556257901390645,
          0.106826801517067,
          0.106826801517067,
          0.11061946902654868,
          0.11061946902654868,
          0.1125158027812895,
          0.1125158027812895,
          0.11314791403286978,
          0.11314791403286978,
          0.11441213653603034,
          0.11441213653603034,
          0.11567635903919089,
          0.11567635903919089,
          0.11630847029077118,
          0.11630847029077118,
          0.11820480404551201,
          0.11820480404551201,
          0.11883691529709228,
          0.11883691529709228,
          0.11946902654867257,
          0.11946902654867257,
          0.12199747155499367,
          0.12199747155499367,
          0.12262958280657396,
          0.12262958280657396,
          0.12326169405815424,
          0.12326169405815424,
          0.12452591656131479,
          0.12452591656131479,
          0.12705436156763592,
          0.12705436156763592,
          0.12895069532237674,
          0.12895069532237674,
          0.13084702907711757,
          0.13084702907711757,
          0.13211125158027812,
          0.13211125158027812,
          0.13274336283185842,
          0.13274336283185842,
          0.13527180783817952,
          0.13527180783817952,
          0.1390644753476612,
          0.1390644753476612,
          0.14285714285714285,
          0.14285714285714285,
          0.14601769911504425,
          0.14601769911504425,
          0.14917825537294563,
          0.14917825537294563,
          0.1529709228824273,
          0.1529709228824273,
          0.15360303413400758,
          0.15360303413400758,
          0.15486725663716813,
          0.15486725663716813,
          0.1561314791403287,
          0.1561314791403287,
          0.1592920353982301,
          0.1592920353982301,
          0.16182048040455121,
          0.16182048040455121,
          0.1624525916561315,
          0.1624525916561315,
          0.16561314791403287,
          0.16561314791403287,
          0.16687737041719342,
          0.16687737041719342,
          0.1675094816687737,
          0.1675094816687737,
          0.17130214917825537,
          0.17130214917825537,
          0.17256637168141592,
          0.17256637168141592,
          0.17699115044247787,
          0.17699115044247787,
          0.17825537294563842,
          0.17825537294563842,
          0.1788874841972187,
          0.1788874841972187,
          0.18015170670037928,
          0.18015170670037928,
          0.18078381795195955,
          0.18078381795195955,
          0.18331226295828065,
          0.18331226295828065,
          0.18394437420986093,
          0.18394437420986093,
          0.18520859671302148,
          0.18520859671302148,
          0.18647281921618206,
          0.18647281921618206,
          0.18836915297092288,
          0.18836915297092288,
          0.18963337547408343,
          0.18963337547408343,
          0.19089759797724398,
          0.19089759797724398,
          0.19279393173198484,
          0.19279393173198484,
          0.19469026548672566,
          0.19469026548672566,
          0.1965865992414665,
          0.1965865992414665,
          0.202275600505689,
          0.202275600505689,
          0.20290771175726927,
          0.20290771175726927,
          0.20480404551201012,
          0.20480404551201012,
          0.20670037926675094,
          0.20670037926675094,
          0.21049304677623262,
          0.21049304677623262,
          0.213653603034134,
          0.213653603034134,
          0.21618204804045513,
          0.21618204804045513,
          0.2168141592920354,
          0.2168141592920354,
          0.21744627054361568,
          0.21744627054361568,
          0.21807838179519595,
          0.21807838179519595,
          0.22313527180783818,
          0.22313527180783818,
          0.2281921618204804,
          0.2281921618204804,
          0.2307206068268015,
          0.2307206068268015,
          0.23324905183312264,
          0.23324905183312264,
          0.23388116308470291,
          0.23388116308470291,
          0.2345132743362832,
          0.2345132743362832,
          0.23514538558786346,
          0.23514538558786346,
          0.2370417193426043,
          0.2370417193426043,
          0.23767383059418457,
          0.23767383059418457,
          0.24336283185840707,
          0.24336283185840707,
          0.2458912768647282,
          0.2458912768647282,
          0.24778761061946902,
          0.24778761061946902,
          0.24968394437420985,
          0.24968394437420985,
          0.2503160556257901,
          0.2503160556257901,
          0.252212389380531,
          0.252212389380531,
          0.25410872313527183,
          0.25410872313527183,
          0.25663716814159293,
          0.25663716814159293,
          0.2572692793931732,
          0.2572692793931732,
          0.25979772439949433,
          0.25979772439949433,
          0.26232616940581543,
          0.26232616940581543,
          0.26422250316055623,
          0.26422250316055623,
          0.26485461441213654,
          0.26485461441213654,
          0.2661188369152971,
          0.2661188369152971,
          0.26738305941845764,
          0.26738305941845764,
          0.27307206068268014,
          0.27307206068268014,
          0.2774968394437421,
          0.2774968394437421,
          0.2800252844500632,
          0.2800252844500632,
          0.2838179519595449,
          0.2838179519595449,
          0.286346396965866,
          0.286346396965866,
          0.28697850821744625,
          0.28697850821744625,
          0.29013906447534765,
          0.29013906447534765,
          0.29266750948166875,
          0.29266750948166875,
          0.2970922882427307,
          0.2970922882427307,
          0.29835651074589126,
          0.29835651074589126,
          0.3027812895069532,
          0.3027812895069532,
          0.31289506953223767,
          0.31289506953223767,
          0.31352718078381797,
          0.31352718078381797,
          0.32364096080910243,
          0.32364096080910243,
          0.324905183312263,
          0.324905183312263,
          0.3268015170670038,
          0.3268015170670038,
          0.32869785082174463,
          0.32869785082174463,
          0.3293299620733249,
          0.3293299620733249,
          0.3305941845764855,
          0.3305941845764855,
          0.3324905183312263,
          0.3324905183312263,
          0.33754740834386854,
          0.33754740834386854,
          0.3388116308470291,
          0.3388116308470291,
          0.3501896333754741,
          0.3501896333754741,
          0.35082174462705434,
          0.35082174462705434,
          0.35651074589127685,
          0.35651074589127685,
          0.35714285714285715,
          0.35714285714285715,
          0.3577749683944374,
          0.3577749683944374,
          0.359039190897598,
          0.359039190897598,
          0.3615676359039191,
          0.3615676359039191,
          0.3640960809102402,
          0.3640960809102402,
          0.3697850821744627,
          0.3697850821744627,
          0.37420986093552466,
          0.37420986093552466,
          0.3786346396965866,
          0.3786346396965866,
          0.3900126422250316,
          0.3900126422250316,
          0.3919089759797724,
          0.3919089759797724,
          0.3950695322376738,
          0.3950695322376738,
          0.404551201011378,
          0.404551201011378,
          0.40644753476611883,
          0.40644753476611883,
          0.41150442477876104,
          0.41150442477876104,
          0.4197218710493047,
          0.4197218710493047,
          0.42857142857142855,
          0.42857142857142855,
          0.4310998735777497,
          0.4310998735777497,
          0.4393173198482933,
          0.4393173198482933,
          0.44310998735777496,
          0.44310998735777496,
          0.4443742098609355,
          0.4443742098609355,
          0.44563843236409606,
          0.44563843236409606,
          0.4481668773704172,
          0.4481668773704172,
          0.44879898862199746,
          0.44879898862199746,
          0.44943109987357777,
          0.44943109987357777,
          0.4633375474083439,
          0.4633375474083439,
          0.46523388116308473,
          0.46523388116308473,
          0.4690265486725664,
          0.4690265486725664,
          0.4740834386852086,
          0.4740834386852086,
          0.4747155499367889,
          0.4747155499367889,
          0.47534766118836913,
          0.47534766118836913,
          0.47914032869785084,
          0.47914032869785084,
          0.4797724399494311,
          0.4797724399494311,
          0.48419721871049304,
          0.48419721871049304,
          0.4892541087231353,
          0.4892541087231353,
          0.4917825537294564,
          0.4917825537294564,
          0.4936788874841972,
          0.4936788874841972,
          0.49873577749683945,
          0.49873577749683945,
          0.5,
          0.5,
          0.504424778761062,
          0.504424778761062,
          0.5056890012642224,
          0.5056890012642224,
          0.5139064475347661,
          0.5139064475347661,
          0.5189633375474083,
          0.5189633375474083,
          0.5195954487989887,
          0.5195954487989887,
          0.5221238938053098,
          0.5221238938053098,
          0.52275600505689,
          0.52275600505689,
          0.5259165613147914,
          0.5259165613147914,
          0.5265486725663717,
          0.5265486725663717,
          0.5316055625790139,
          0.5316055625790139,
          0.5328697850821744,
          0.5328697850821744,
          0.5335018963337548,
          0.5335018963337548,
          0.5360303413400759,
          0.5360303413400759,
          0.5366624525916561,
          0.5366624525916561,
          0.5404551201011378,
          0.5404551201011378,
          0.5429835651074589,
          0.5429835651074589,
          0.5518331226295828,
          0.5518331226295828,
          0.5581542351453855,
          0.5581542351453855,
          0.5594184576485461,
          0.5594184576485461,
          0.5644753476611883,
          0.5644753476611883,
          0.5720606826801518,
          0.5720606826801518,
          0.5796460176991151,
          0.5796460176991151,
          0.5853350189633375,
          0.5853350189633375,
          0.588495575221239,
          0.588495575221239,
          0.5897597977243995,
          0.5897597977243995,
          0.5935524652338812,
          0.5935524652338812,
          0.6055625790139064,
          0.6055625790139064,
          0.6074589127686473,
          0.6074589127686473,
          0.6087231352718079,
          0.6087231352718079,
          0.6106194690265486,
          0.6106194690265486,
          0.6118836915297092,
          0.6118836915297092,
          0.6137800252844501,
          0.6137800252844501,
          0.618204804045512,
          0.618204804045512,
          0.6194690265486725,
          0.6194690265486725,
          0.629582806573957,
          0.629582806573957,
          0.6314791403286979,
          0.6314791403286979,
          0.634007585335019,
          0.634007585335019,
          0.6365360303413401,
          0.6365360303413401,
          0.6390644753476612,
          0.6390644753476612,
          0.640960809102402,
          0.640960809102402,
          0.6434892541087232,
          0.6434892541087232,
          0.6472819216182049,
          0.6472819216182049,
          0.6485461441213654,
          0.6485461441213654,
          0.6523388116308471,
          0.6523388116308471,
          0.6548672566371682,
          0.6548672566371682,
          0.6624525916561315,
          0.6624525916561315,
          0.6637168141592921,
          0.6637168141592921,
          0.6738305941845765,
          0.6738305941845765,
          0.6801517067003793,
          0.6801517067003793,
          0.6871049304677623,
          0.6871049304677623,
          0.6883691529709229,
          0.6883691529709229,
          0.6915297092288243,
          0.6915297092288243,
          0.6927939317319848,
          0.6927939317319848,
          0.6940581542351454,
          0.6940581542351454,
          0.7041719342604298,
          0.7041719342604298,
          0.7085967130214917,
          0.7085967130214917,
          0.7117572692793932,
          0.7117572692793932,
          0.7149178255372945,
          0.7149178255372945,
          0.718078381795196,
          0.718078381795196,
          0.7225031605562579,
          0.7225031605562579,
          0.7231352718078382,
          0.7231352718078382,
          0.7256637168141593,
          0.7256637168141593,
          0.7338811630847029,
          0.7338811630847029,
          0.7351453855878635,
          0.7351453855878635,
          0.7427307206068268,
          0.7427307206068268,
          0.7433628318584071,
          0.7433628318584071,
          0.7509481668773704,
          0.7509481668773704,
          0.7515802781289507,
          0.7515802781289507,
          0.7528445006321113,
          0.7528445006321113,
          0.7566371681415929,
          0.7566371681415929,
          0.7604298356510746,
          0.7604298356510746,
          0.7610619469026548,
          0.7610619469026548,
          0.7635903919089759,
          0.7635903919089759,
          0.7812895069532237,
          0.7812895069532237,
          0.7857142857142857,
          0.7857142857142857,
          0.8078381795195955,
          0.8078381795195955,
          0.809102402022756,
          0.809102402022756,
          0.8128950695322377,
          0.8128950695322377,
          0.8249051833122629,
          0.8249051833122629,
          0.831858407079646,
          0.831858407079646,
          0.8394437420986094,
          0.8394437420986094,
          0.8514538558786346,
          0.8514538558786346,
          0.8539823008849557,
          0.8539823008849557,
          0.8792667509481669,
          0.8792667509481669,
          0.8988621997471555,
          0.8988621997471555,
          0.9026548672566371,
          0.9026548672566371,
          0.9152970922882427,
          0.9152970922882427,
          0.9178255372945638,
          0.9178255372945638,
          0.9329962073324906,
          0.9329962073324906,
          0.9374209860935525,
          0.9374209860935525,
          0.9405815423514539,
          0.9405815423514539,
          0.9424778761061947,
          0.9424778761061947,
          0.943109987357775,
          0.943109987357775,
          0.9589127686472819,
          0.9589127686472819,
          0.9633375474083439,
          0.9633375474083439,
          0.9721871049304678,
          0.9721871049304678,
          1
         ],
         "y": [
          0,
          0,
          0,
          0.004784688995215311,
          0.004784688995215311,
          0.009569377990430622,
          0.009569377990430622,
          0.011961722488038277,
          0.011961722488038277,
          0.023923444976076555,
          0.023923444976076555,
          0.02631578947368421,
          0.02631578947368421,
          0.028708133971291867,
          0.028708133971291867,
          0.03110047846889952,
          0.03110047846889952,
          0.03349282296650718,
          0.03349282296650718,
          0.03588516746411483,
          0.03588516746411483,
          0.0430622009569378,
          0.0430622009569378,
          0.045454545454545456,
          0.045454545454545456,
          0.04784688995215311,
          0.04784688995215311,
          0.050239234449760764,
          0.050239234449760764,
          0.05263157894736842,
          0.05263157894736842,
          0.05502392344497608,
          0.05502392344497608,
          0.05741626794258373,
          0.05741626794258373,
          0.06220095693779904,
          0.06220095693779904,
          0.06937799043062201,
          0.06937799043062201,
          0.07177033492822966,
          0.07177033492822966,
          0.07894736842105263,
          0.07894736842105263,
          0.08133971291866028,
          0.08133971291866028,
          0.0861244019138756,
          0.0861244019138756,
          0.08851674641148326,
          0.08851674641148326,
          0.09569377990430622,
          0.09569377990430622,
          0.09808612440191387,
          0.09808612440191387,
          0.1076555023923445,
          0.1076555023923445,
          0.11004784688995216,
          0.11004784688995216,
          0.11483253588516747,
          0.11483253588516747,
          0.11961722488038277,
          0.11961722488038277,
          0.12200956937799043,
          0.12200956937799043,
          0.12679425837320574,
          0.12679425837320574,
          0.13875598086124402,
          0.13875598086124402,
          0.1507177033492823,
          0.1507177033492823,
          0.15550239234449761,
          0.15550239234449761,
          0.16507177033492823,
          0.16507177033492823,
          0.16985645933014354,
          0.16985645933014354,
          0.17942583732057416,
          0.17942583732057416,
          0.18181818181818182,
          0.18181818181818182,
          0.18421052631578946,
          0.18421052631578946,
          0.18660287081339713,
          0.18660287081339713,
          0.18899521531100477,
          0.18899521531100477,
          0.19138755980861244,
          0.19138755980861244,
          0.1937799043062201,
          0.1937799043062201,
          0.19617224880382775,
          0.19617224880382775,
          0.20095693779904306,
          0.20095693779904306,
          0.20334928229665072,
          0.20334928229665072,
          0.20574162679425836,
          0.20574162679425836,
          0.21291866028708134,
          0.21291866028708134,
          0.215311004784689,
          0.215311004784689,
          0.21770334928229665,
          0.21770334928229665,
          0.22248803827751196,
          0.22248803827751196,
          0.22727272727272727,
          0.22727272727272727,
          0.22966507177033493,
          0.22966507177033493,
          0.23205741626794257,
          0.23205741626794257,
          0.23684210526315788,
          0.23684210526315788,
          0.23923444976076555,
          0.23923444976076555,
          0.24162679425837322,
          0.24162679425837322,
          0.24401913875598086,
          0.24401913875598086,
          0.24880382775119617,
          0.24880382775119617,
          0.2511961722488038,
          0.2511961722488038,
          0.2535885167464115,
          0.2535885167464115,
          0.25598086124401914,
          0.25598086124401914,
          0.2583732057416268,
          0.2583732057416268,
          0.2607655502392344,
          0.2607655502392344,
          0.26555023923444976,
          0.26555023923444976,
          0.2679425837320574,
          0.2679425837320574,
          0.2703349282296651,
          0.2703349282296651,
          0.2727272727272727,
          0.2727272727272727,
          0.2751196172248804,
          0.2751196172248804,
          0.2799043062200957,
          0.2799043062200957,
          0.2822966507177033,
          0.2822966507177033,
          0.284688995215311,
          0.284688995215311,
          0.28708133971291866,
          0.28708133971291866,
          0.2894736842105263,
          0.2894736842105263,
          0.291866028708134,
          0.291866028708134,
          0.2966507177033493,
          0.2966507177033493,
          0.29904306220095694,
          0.29904306220095694,
          0.3014354066985646,
          0.3014354066985646,
          0.3038277511961722,
          0.3038277511961722,
          0.3062200956937799,
          0.3062200956937799,
          0.3133971291866029,
          0.3133971291866029,
          0.3157894736842105,
          0.3157894736842105,
          0.3181818181818182,
          0.3181818181818182,
          0.32057416267942584,
          0.32057416267942584,
          0.3253588516746411,
          0.3253588516746411,
          0.33014354066985646,
          0.33014354066985646,
          0.33253588516746413,
          0.33253588516746413,
          0.3349282296650718,
          0.3349282296650718,
          0.3397129186602871,
          0.3397129186602871,
          0.34210526315789475,
          0.34210526315789475,
          0.34688995215311,
          0.34688995215311,
          0.35167464114832536,
          0.35167464114832536,
          0.35406698564593303,
          0.35406698564593303,
          0.35645933014354064,
          0.35645933014354064,
          0.3588516746411483,
          0.3588516746411483,
          0.361244019138756,
          0.361244019138756,
          0.36363636363636365,
          0.36363636363636365,
          0.3660287081339713,
          0.3660287081339713,
          0.3684210526315789,
          0.3684210526315789,
          0.3708133971291866,
          0.3708133971291866,
          0.37320574162679426,
          0.37320574162679426,
          0.37799043062200954,
          0.37799043062200954,
          0.3803827751196172,
          0.3803827751196172,
          0.3827751196172249,
          0.3827751196172249,
          0.38516746411483255,
          0.38516746411483255,
          0.3875598086124402,
          0.3875598086124402,
          0.39473684210526316,
          0.39473684210526316,
          0.39712918660287083,
          0.39712918660287083,
          0.39952153110047844,
          0.39952153110047844,
          0.4043062200956938,
          0.4043062200956938,
          0.40669856459330145,
          0.40669856459330145,
          0.4090909090909091,
          0.4090909090909091,
          0.41148325358851673,
          0.41148325358851673,
          0.4138755980861244,
          0.4138755980861244,
          0.41626794258373206,
          0.41626794258373206,
          0.41866028708133973,
          0.41866028708133973,
          0.42105263157894735,
          0.42105263157894735,
          0.423444976076555,
          0.423444976076555,
          0.4258373205741627,
          0.4258373205741627,
          0.42822966507177035,
          0.42822966507177035,
          0.430622009569378,
          0.430622009569378,
          0.43301435406698563,
          0.43301435406698563,
          0.43779904306220097,
          0.43779904306220097,
          0.44976076555023925,
          0.44976076555023925,
          0.45215311004784686,
          0.45215311004784686,
          0.45454545454545453,
          0.45454545454545453,
          0.4569377990430622,
          0.4569377990430622,
          0.46172248803827753,
          0.46172248803827753,
          0.46411483253588515,
          0.46411483253588515,
          0.4688995215311005,
          0.4688995215311005,
          0.47368421052631576,
          0.47368421052631576,
          0.47607655502392343,
          0.47607655502392343,
          0.4784688995215311,
          0.4784688995215311,
          0.48325358851674644,
          0.48325358851674644,
          0.48564593301435405,
          0.48564593301435405,
          0.4904306220095694,
          0.4904306220095694,
          0.49282296650717705,
          0.49282296650717705,
          0.49760765550239233,
          0.49760765550239233,
          0.5,
          0.5,
          0.507177033492823,
          0.507177033492823,
          0.5095693779904307,
          0.5095693779904307,
          0.5119617224880383,
          0.5119617224880383,
          0.5143540669856459,
          0.5143540669856459,
          0.5167464114832536,
          0.5167464114832536,
          0.5263157894736842,
          0.5263157894736842,
          0.5287081339712919,
          0.5287081339712919,
          0.5334928229665071,
          0.5334928229665071,
          0.5406698564593302,
          0.5406698564593302,
          0.5430622009569378,
          0.5430622009569378,
          0.5454545454545454,
          0.5454545454545454,
          0.5478468899521531,
          0.5478468899521531,
          0.5526315789473685,
          0.5526315789473685,
          0.5574162679425837,
          0.5574162679425837,
          0.562200956937799,
          0.562200956937799,
          0.5645933014354066,
          0.5645933014354066,
          0.5741626794258373,
          0.5741626794258373,
          0.5765550239234449,
          0.5765550239234449,
          0.5789473684210527,
          0.5789473684210527,
          0.5813397129186603,
          0.5813397129186603,
          0.5861244019138756,
          0.5861244019138756,
          0.5885167464114832,
          0.5885167464114832,
          0.5956937799043063,
          0.5956937799043063,
          0.5980861244019139,
          0.5980861244019139,
          0.6004784688995215,
          0.6004784688995215,
          0.6028708133971292,
          0.6028708133971292,
          0.6100478468899522,
          0.6100478468899522,
          0.6148325358851675,
          0.6148325358851675,
          0.6172248803827751,
          0.6172248803827751,
          0.6196172248803827,
          0.6196172248803827,
          0.6220095693779905,
          0.6220095693779905,
          0.6291866028708134,
          0.6291866028708134,
          0.631578947368421,
          0.631578947368421,
          0.6339712918660287,
          0.6339712918660287,
          0.6363636363636364,
          0.6363636363636364,
          0.638755980861244,
          0.638755980861244,
          0.6411483253588517,
          0.6411483253588517,
          0.6435406698564593,
          0.6435406698564593,
          0.645933014354067,
          0.645933014354067,
          0.6483253588516746,
          0.6483253588516746,
          0.6507177033492823,
          0.6507177033492823,
          0.65311004784689,
          0.65311004784689,
          0.6578947368421053,
          0.6578947368421053,
          0.6602870813397129,
          0.6602870813397129,
          0.6626794258373205,
          0.6626794258373205,
          0.6650717703349283,
          0.6650717703349283,
          0.6674641148325359,
          0.6674641148325359,
          0.6722488038277512,
          0.6722488038277512,
          0.6770334928229665,
          0.6770334928229665,
          0.6794258373205742,
          0.6794258373205742,
          0.6818181818181818,
          0.6818181818181818,
          0.6889952153110048,
          0.6889952153110048,
          0.6913875598086124,
          0.6913875598086124,
          0.6961722488038278,
          0.6961722488038278,
          0.7009569377990431,
          0.7009569377990431,
          0.7033492822966507,
          0.7033492822966507,
          0.7057416267942583,
          0.7057416267942583,
          0.7081339712918661,
          0.7081339712918661,
          0.715311004784689,
          0.715311004784689,
          0.7177033492822966,
          0.7177033492822966,
          0.7200956937799043,
          0.7200956937799043,
          0.722488038277512,
          0.722488038277512,
          0.7248803827751196,
          0.7248803827751196,
          0.7272727272727273,
          0.7272727272727273,
          0.7320574162679426,
          0.7320574162679426,
          0.7344497607655502,
          0.7344497607655502,
          0.7368421052631579,
          0.7368421052631579,
          0.7392344497607656,
          0.7392344497607656,
          0.7416267942583732,
          0.7416267942583732,
          0.7464114832535885,
          0.7464114832535885,
          0.7488038277511961,
          0.7488038277511961,
          0.7511961722488039,
          0.7511961722488039,
          0.7535885167464115,
          0.7535885167464115,
          0.7559808612440191,
          0.7559808612440191,
          0.7583732057416268,
          0.7583732057416268,
          0.7607655502392344,
          0.7607655502392344,
          0.7631578947368421,
          0.7631578947368421,
          0.7655502392344498,
          0.7655502392344498,
          0.7679425837320574,
          0.7679425837320574,
          0.7703349282296651,
          0.7703349282296651,
          0.7727272727272727,
          0.7727272727272727,
          0.7751196172248804,
          0.7751196172248804,
          0.777511961722488,
          0.777511961722488,
          0.7822966507177034,
          0.7822966507177034,
          0.784688995215311,
          0.784688995215311,
          0.7870813397129187,
          0.7870813397129187,
          0.7918660287081339,
          0.7918660287081339,
          0.7942583732057417,
          0.7942583732057417,
          0.7966507177033493,
          0.7966507177033493,
          0.7990430622009569,
          0.7990430622009569,
          0.8014354066985646,
          0.8014354066985646,
          0.8038277511961722,
          0.8038277511961722,
          0.80622009569378,
          0.80622009569378,
          0.8086124401913876,
          0.8086124401913876,
          0.8110047846889952,
          0.8110047846889952,
          0.8133971291866029,
          0.8133971291866029,
          0.8157894736842105,
          0.8157894736842105,
          0.8181818181818182,
          0.8181818181818182,
          0.8205741626794258,
          0.8205741626794258,
          0.8229665071770335,
          0.8229665071770335,
          0.8253588516746412,
          0.8253588516746412,
          0.8277511961722488,
          0.8277511961722488,
          0.8301435406698564,
          0.8301435406698564,
          0.8325358851674641,
          0.8325358851674641,
          0.8349282296650717,
          0.8349282296650717,
          0.8397129186602871,
          0.8397129186602871,
          0.8444976076555024,
          0.8444976076555024,
          0.84688995215311,
          0.84688995215311,
          0.8492822966507177,
          0.8492822966507177,
          0.8516746411483254,
          0.8516746411483254,
          0.854066985645933,
          0.854066985645933,
          0.8588516746411483,
          0.8588516746411483,
          0.861244019138756,
          0.861244019138756,
          0.8636363636363636,
          0.8636363636363636,
          0.8660287081339713,
          0.8660287081339713,
          0.868421052631579,
          0.868421052631579,
          0.8708133971291866,
          0.8708133971291866,
          0.8732057416267942,
          0.8732057416267942,
          0.8779904306220095,
          0.8779904306220095,
          0.8803827751196173,
          0.8803827751196173,
          0.8851674641148325,
          0.8851674641148325,
          0.8875598086124402,
          0.8875598086124402,
          0.8899521531100478,
          0.8899521531100478,
          0.8923444976076556,
          0.8923444976076556,
          0.8995215311004785,
          0.8995215311004785,
          0.9019138755980861,
          0.9019138755980861,
          0.9043062200956937,
          0.9043062200956937,
          0.9066985645933014,
          0.9066985645933014,
          0.9090909090909091,
          0.9090909090909091,
          0.9114832535885168,
          0.9114832535885168,
          0.916267942583732,
          0.916267942583732,
          0.9186602870813397,
          0.9186602870813397,
          0.9210526315789473,
          0.9210526315789473,
          0.9234449760765551,
          0.9234449760765551,
          0.9258373205741627,
          0.9258373205741627,
          0.9282296650717703,
          0.9282296650717703,
          0.9330143540669856,
          0.9330143540669856,
          0.9354066985645934,
          0.9354066985645934,
          0.937799043062201,
          0.937799043062201,
          0.9401913875598086,
          0.9401913875598086,
          0.9425837320574163,
          0.9425837320574163,
          0.9449760765550239,
          0.9449760765550239,
          0.9473684210526315,
          0.9473684210526315,
          0.9521531100478469,
          0.9521531100478469,
          0.9545454545454546,
          0.9545454545454546,
          0.9569377990430622,
          0.9569377990430622,
          0.9593301435406698,
          0.9593301435406698,
          0.9617224880382775,
          0.9617224880382775,
          0.9641148325358851,
          0.9641148325358851,
          0.9665071770334929,
          0.9665071770334929,
          0.9688995215311005,
          0.9688995215311005,
          0.9712918660287081,
          0.9712918660287081,
          0.9736842105263158,
          0.9736842105263158,
          0.9760765550239234,
          0.9760765550239234,
          0.9784688995215312,
          0.9784688995215312,
          0.9808612440191388,
          0.9808612440191388,
          0.9832535885167464,
          0.9832535885167464,
          0.9856459330143541,
          0.9856459330143541,
          0.9880382775119617,
          0.9880382775119617,
          0.9904306220095693,
          0.9904306220095693,
          0.992822966507177,
          0.992822966507177,
          0.9952153110047847,
          0.9952153110047847,
          0.9976076555023924,
          0.9976076555023924,
          1,
          1
         ]
        },
        {
         "line": {
          "color": "aqua"
         },
         "mode": "lines",
         "name": "Random model",
         "type": "scatter",
         "x": [
          0,
          1
         ],
         "y": [
          0,
          1
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#f2f5fa"
            },
            "error_y": {
             "color": "#f2f5fa"
            },
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "baxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#506784"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "header": {
             "fill": {
              "color": "#2a3f5f"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#f2f5fa",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#f2f5fa"
          },
          "geo": {
           "bgcolor": "rgb(17,17,17)",
           "lakecolor": "rgb(17,17,17)",
           "landcolor": "rgb(17,17,17)",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#506784"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "dark"
          },
          "paper_bgcolor": "rgb(17,17,17)",
          "plot_bgcolor": "rgb(17,17,17)",
          "polar": {
           "angularaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "bgcolor": "rgb(17,17,17)",
           "radialaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           },
           "yaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           },
           "zaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#f2f5fa"
           }
          },
          "sliderdefaults": {
           "bgcolor": "#C8D4E3",
           "bordercolor": "rgb(17,17,17)",
           "borderwidth": 1,
           "tickwidth": 0
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "bgcolor": "rgb(17,17,17)",
           "caxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "updatemenudefaults": {
           "bgcolor": "#506784",
           "borderwidth": 0
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "color": "white"
         },
         "text": "ROC curve"
        },
        "width": 700,
        "xaxis": {
         "title": {
          "text": "False Positive Rate"
         }
        },
        "yaxis": {
         "title": {
          "text": "True Positive Rate"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under Curve = 0.672703984418004\n"
     ]
    }
   ],
   "source": [
    "# find fpr, tpr, thresholds with sklearn roc_curve function\n",
    "fpr, tpr, thresholds = roc_curve(lr_target_valid, probabilities_one_valid)\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot ROC curve\n",
    "fig.add_trace(go.Scatter(x=fpr, y=tpr,\n",
    "                    mode='lines',\n",
    "                    name='High quality model',\n",
    "                    line=dict(color='deeppink')))  # Set ROC curve color\n",
    "\n",
    "# Plot line of random model\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1],\n",
    "                    mode='lines',\n",
    "                    name='Random model',\n",
    "                    line=dict(color='aqua')))  # Set Random model color\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title='ROC curve',\n",
    "    xaxis=dict(title='False Positive Rate'),\n",
    "    yaxis=dict(title='True Positive Rate'),\n",
    "    width=700,\n",
    "    height=500,\n",
    "    template='plotly_dark',  # Change the template to 'plotly_dark' for a dark background \n",
    "    title_font_color='white'  # Change title font color\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n",
    "\n",
    "# Calculate AUC score\n",
    "auc_roc = roc_auc_score(lr_target_valid, probabilities_one_valid)\n",
    "\n",
    "# Print AUC Score\n",
    "print('Area Under Curve =', auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the ROC curve\n",
    "\n",
    "The model is far from perfect, but it's better than the random model. The area under the curve is 0.67, which is better than the 0.5 of a random model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Testing\n",
    "\n",
    "After making adjustments to both the class weights and thresholds, the random forest model is the most effective. This model was able to reach an F1 level of 62.18% with the validation set using upscaling methods. The validaiton set will now be incorporated into the training set. Once this has been accomplished, the same methods used to upsample will be used. These results will be used to test  the test set.\n",
    "\n",
    "### Incorporate Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_zeros length: 6386\n",
      "target_ones length: 1614\n",
      "Ratio: 3.96\n",
      "Ratio Imbalanced\n"
     ]
    }
   ],
   "source": [
    "# Combine random forest validation and training sets for features and target\n",
    "rf_features_valid_train = pd.concat([rf_features_valid, rf_features_train])\n",
    "rf_target_valid_train = pd.concat([rf_target_valid, rf_target_train])\n",
    "\n",
    "# 1. Split data by target\n",
    "\n",
    "# Split target data\n",
    "target_zeros = rf_target_valid_train[rf_target_valid_train == 0]\n",
    "target_ones = rf_target_valid_train[rf_target_valid_train == 1]\n",
    "\n",
    "# Split features data based on target data for both random forest and logistic regression\n",
    "features_zeros = rf_features_valid_train[rf_target_valid_train == 0]\n",
    "features_ones = rf_features_valid_train[rf_target_valid_train == 1]\n",
    "\n",
    "# Print lengths for target zeros and target ones\n",
    "print('target_zeros length:', len(target_zeros))\n",
    "print('target_ones length:', len(target_ones))\n",
    "print('Ratio:', round(len(target_zeros)/len(target_ones), 2))\n",
    "print('Ratio Imbalanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations where the customer stayed: 6386\n",
      "Number of observations where the customer left: 4842\n",
      "Ratio: 1.32\n",
      "Ratio Balanced\n"
     ]
    }
   ],
   "source": [
    "# 2. Duplicate appropriate observations\n",
    "\n",
    "# Find ratio of target zeros to target ones\n",
    "ratio= int(len(target_zeros) / len(target_ones))\n",
    "\n",
    "# Duplicate target ones by ratio and concatenate for target for random forests\n",
    "target_upsampled = pd.concat([target_zeros] + [target_ones] * ratio)\n",
    "\n",
    "# Repeat for features\n",
    "features_upsampled = pd.concat([features_zeros] + [features_ones] * ratio)\n",
    "\n",
    "# Print lengths for target zeros and target ones\n",
    "print('Number of observations where the customer stayed:', len(target_zeros))\n",
    "print('Number of observations where the customer left:', len(target_ones) * ratio)\n",
    "print('Ratio:', round(len(target_zeros)/(len(target_ones) * ratio), 2))\n",
    "print('Ratio Balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Shuffle Data\n",
    "features_upsampled, target_upsampled = shuffle(features_upsampled, target_upsampled, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the best model on test set: 60.61 %\n",
      "Accuracy of the best model on test set: 83.3 %\n"
     ]
    }
   ],
   "source": [
    "# 4. Train Model\n",
    "\n",
    "# Use estimators = 100 and max_depth = 10 from previous model\n",
    "model = RandomForestClassifier(max_depth=10, random_state=12345, n_estimators=100) # set number of trees\n",
    "\n",
    "# Train model on combined training and validation upsampled set\n",
    "model.fit(features_upsampled, target_upsampled)\n",
    "\n",
    "# Calculate f1 and accuracy scores on test set\n",
    "f1 = f1_score(rf_target_test, model.predict(rf_features_test))\n",
    "accuracy = accuracy_score(rf_target_test, model.predict(rf_features_test))\n",
    "\n",
    "# Print F1 score and Accuracy score\n",
    "print(\"F1 score of the best model on test set:\", round(f1*100,2), \"%\")\n",
    "print(\"Accuracy of the best model on test set:\", round(accuracy*100,2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "The data was saved, cleaned and pre-processed. This meant installing the appropriate packages, tidying up column names, checking for null and duplicate values, and encoding the data according to datatypes.\n",
    "\n",
    "### Class Balances\n",
    "\n",
    "Class balances were examined and noted that a constant model would produce results similar to the models that were tested. To overcome these problems, the F1 score became the metric of value.\n",
    "\n",
    "### Improving Models\n",
    "\n",
    "The models were improved with changes to class weights and thresholds. By upsampling a downsampling, the F1 scores on the random forests were increased to over 60%. The logistic regression model saw some improvement, but saw its biggest jump in F1 when adjusting threshold levels. At a threshold of 0.22, the F1 score was 41%. After creating an AUC-ROC graph it was determined that these logistic regression models were better than random, but not by a significatn amount.\n",
    "\n",
    "### Final Testing\n",
    "Ultimately, the random forest produced the highest F1 score at 62%. The hyperparameters used 100 estimators and a depth of 10. The model was further improved by combining training and validation sets and upsampling the data. Once this was done, the model was tested on the test set. This yielded an F1 score of 60% and an accuracy of 83.3%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qtconsole",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
